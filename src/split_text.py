# qwen_embedding_rag.py - è°ƒç”¨é€šä¹‰åƒé—® Embedding API + Chroma å­˜å‚¨
import json
import os
import time
import logging
import dotenv
from typing import List, Optional, Dict, Any

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv.load_dotenv()

# æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# å‘é‡åº“
from langchain_chroma import Chroma
from chromadb.config import Settings

# ç¦ç”¨ Chroma telemetry
logger.info("ğŸ”§ å·²è®¾ç½®ç¦ç”¨ Chroma telemetry")
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# é€šä¹‰åƒé—® SDK
import dashscope
from dashscope import TextEmbedding
import requests  # æ–°å¢ï¼Œç”¨äºç›´æ¥HTTPè¯·æ±‚

# ================== é…ç½® ==================
# ä»ç¯å¢ƒå˜é‡è·å– API Key
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
if not DASHSCOPE_API_KEY:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")

dashscope.api_key = DASHSCOPE_API_KEY

# é…ç½®å‚æ•°
# ä½¿ç”¨æµ‹è¯•æˆåŠŸçš„æ¨¡å‹ç‰ˆæœ¬
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-v1")  # é€šä¹‰åƒé—®æ¨¡å‹å
VECTOR_DB_PATH = os.getenv("VECTOR_DB_PATH", "vectorstore_qwen")
os.makedirs(VECTOR_DB_PATH, exist_ok=True)

# åˆ†å—é…ç½®
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "500"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "50"))
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "5"))  # é€šä¹‰åƒé—®å…è´¹ç‰ˆé™æµï¼šæ¯åˆ†é’Ÿ 5 æ¬¡è¯·æ±‚
REQUEST_INTERVAL = int(os.getenv("REQUEST_INTERVAL", "12"))  # è¯·æ±‚é—´éš”æ—¶é—´(ç§’)
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))  # æœ€å¤§é‡è¯•æ¬¡æ•°

current_dir = os.path.dirname(os.path.abspath(__file__))
INPUT_JSON = os.path.join(current_dir, "docs.json")

# æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# ================== é€šä¹‰åƒé—® Embedding ç±» ==================
class QwenEmbeddings:
    """å°è£…é€šä¹‰åƒé—® Embedding API è°ƒç”¨"""
    def __init__(self, model: str = EMBEDDING_MODEL):
        self.model = model
        self.MAX_RETRIES = MAX_RETRIES
        self.REQUEST_INTERVAL = REQUEST_INTERVAL
        self.BATCH_SIZE = BATCH_SIZE

    def _call_api(self, texts: List[str]) -> List[List[float]]:
        """è°ƒç”¨ DashScope Embedding APIï¼Œä»…ä½¿ç”¨HTTPè°ƒç”¨æ–¹å¼"""
        retry_count = 0
        while retry_count < self.MAX_RETRIES:
            try:
                logger.info(f"ğŸ“¤ å‘é€HTTPè¯·æ±‚: {len(texts)} ä¸ªæ–‡æœ¬ (é‡è¯•: {retry_count}/{self.MAX_RETRIES-1})")
                
                # æ ¹æ®é˜¿é‡Œäº‘å®˜æ–¹æ–‡æ¡£ï¼Œæ‰€æœ‰APIè°ƒç”¨æœ¬è´¨éƒ½æ˜¯HTTPè¯·æ±‚
                url = "https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings"
                headers = {
                    "Authorization": f"Bearer {dashscope.api_key}",
                    "Content-Type": "application/json"
                }
                
                data = {
                    "model": "text-embedding-v4",
                    "input": texts,
                    "dimensions": 1024,
                    "encoding_format": "float"
                }
                response = requests.post(url, json=data, headers=headers)
                logger.info(f"ğŸ“¥ æ”¶åˆ° HTTP å“åº”: çŠ¶æ€ç  {response.status_code}")
                if response.status_code == 200:
                        result = response.json()
                        # æ£€æŸ¥å“åº”ç»“æ„
                        if "data" not in result:
                            raise Exception("HTTP API å“åº”æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘ data å­—æ®µ")
                        embeddings_data = [item["embedding"] for item in result["data"]]
                        results = embeddings_data
                        logger.info(f"âœ… æˆåŠŸè·å– {len(results)} ä¸ªåµŒå…¥å‘é‡")
                        return results
                elif response.status_code in [429, 500, 502, 503, 504]:
                        # é™æµæˆ–æœåŠ¡å™¨é”™è¯¯ï¼Œé‡è¯•
                        raise Exception(f"HTTP API é”™è¯¯: çŠ¶æ€ç  {response.status_code}, å“åº”: {response.text}")
                else:
                        raise Exception(f"HTTP API é”™è¯¯: çŠ¶æ€ç  {response.status_code}, å“åº”: {response.text}")
            except Exception as e:
                retry_count += 1
                logger.error(f"âŒ Embedding è°ƒç”¨å¤±è´¥: {str(e)}, é‡è¯• {retry_count}/{self.MAX_RETRIES-1}")
                if retry_count < self.MAX_RETRIES:
                    wait_time = self.REQUEST_INTERVAL * (2 ** retry_count)
                    time.sleep(wait_time)
                else:
                    raise

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”Ÿæˆæ–‡æ¡£åµŒå…¥ï¼ˆè‡ªåŠ¨åˆ†æ‰¹ï¼‰"""
        all_embeddings = []
        for i in range(0, len(texts), self.BATCH_SIZE):
            batch = texts[i:i+self.BATCH_SIZE]
            batch_num = i // self.BATCH_SIZE + 1
            logger.info(f"ğŸŒ è°ƒç”¨é€šä¹‰åƒé—® Embedding (æ‰¹æ¬¡: {batch_num}/{(len(texts)-1)//self.BATCH_SIZE + 1}, æ•°é‡: {len(batch)})")
            batch_embeds = self._call_api(batch)
            all_embeddings.extend(batch_embeds)
            logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} å¤„ç†å®Œæˆ")
            # é¿å…é™æµ
            if i + BATCH_SIZE < len(texts):  # ä¸æ˜¯æœ€åä¸€æ‰¹æ‰éœ€è¦ç­‰å¾…
                time.sleep(REQUEST_INTERVAL)
        return all_embeddings

    def embed_query(self, text: str) -> List[float]:
        """ä¸ºæŸ¥è¯¢ç”ŸæˆåµŒå…¥"""
        return self._call_api([text])[0]


# ================== ä¸»ç¨‹åº ==================
def main():
    logger.info("ğŸš€ å¼€å§‹ä½¿ç”¨é€šä¹‰åƒé—® Embedding è¿›è¡Œå‘é‡åŒ–")

    # 1. è¯»å– docs.json
    try:
        with open(INPUT_JSON, "r", encoding="utf-8") as f:
            docs = json.load(f)
        logger.info(f"âœ… åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
    except FileNotFoundError:
        logger.error("âŒ æ‰¾ä¸åˆ° ../docs.json")
        return
    except Exception as e:
        logger.error(f"âŒ è¯»å–å¤±è´¥: {e}")
        return

    # 2. åˆ†å—
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
    )
    all_chunks = []

    for doc in docs:
        if doc.get("status") != "success":
            logger.warning(f"âš ï¸ è·³è¿‡çŠ¶æ€ésuccessçš„æ–‡æ¡£: {doc.get('filename', 'æœªçŸ¥æ–‡ä»¶å')}")
            continue
        content = doc.get("content", "").strip()
        if not content:
            logger.warning(f"âš ï¸ è·³è¿‡ç©ºå†…å®¹æ–‡æ¡£: {doc.get('filename', 'æœªçŸ¥æ–‡ä»¶å')}")
            continue
        if len(content) < 10:
            logger.warning(f"âš ï¸ è·³è¿‡å†…å®¹è¿‡çŸ­æ–‡æ¡£: {doc.get('filename', 'æœªçŸ¥æ–‡ä»¶å')} (é•¿åº¦: {len(content)})")
            continue

        chunks = splitter.split_text(content)
        logger.info(f"âœ‚ï¸  {doc.get('filename', 'æœªçŸ¥æ–‡ä»¶å')} â†’ ç”Ÿæˆ {len(chunks)} ä¸ªå—")
        for i, chunk in enumerate(chunks):
            clean_chunk = chunk.replace("\r", "").strip()
            if len(clean_chunk) < 5:
                logger.warning(f"âš ï¸ è·³è¿‡è¿‡çŸ­å— ({len(clean_chunk)} å­—ç¬¦): {clean_chunk[:20]}...")
                continue
            doc_obj = Document(
                page_content=clean_chunk,
                metadata={"source": doc["filename"], "chunk_id": i}
            )
            all_chunks.append(doc_obj)
            logger.info(f"âœ… æ·»åŠ å— {i+1}/{len(chunks)}: {clean_chunk[:30]}...")

    if not all_chunks:
        logger.error("âŒ æ²¡æœ‰å¯å¤„ç†çš„æ–‡æœ¬å—")
        return
    logger.info(f"âœ… å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬å—")

    # 3. åˆå§‹åŒ–é€šä¹‰åƒé—® Embedding
    try:
        embeddings = QwenEmbeddings()
        # æµ‹è¯•è°ƒç”¨
        test_embed = embeddings.embed_query("æµ‹è¯•")
        logger.info(f"âœ… é€šä¹‰åƒé—® Embedding åˆå§‹åŒ–æˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(test_embed)}")
    except Exception as e:
        logger.error("âŒ åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key æˆ–ç½‘ç»œ")
        return

    # 4. åˆ›å»º Chroma å‘é‡åº“
    try:
        logger.info(f"ğŸ”„ å¼€å§‹åˆ›å»ºå‘é‡åº“ï¼Œå…± {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
        # å®šä¹‰å‘é‡åº“ç»å¯¹è·¯å¾„
        abs_vector_path = os.path.abspath(VECTOR_DB_PATH)
        logger.info(f"ğŸ” å‘é‡åº“ç»å¯¹è·¯å¾„: {abs_vector_path}")

        # ç›´æ¥ä½¿ç”¨chromadbå®¢æˆ·ç«¯åˆ›å»ºå‘é‡åº“
        from chromadb import PersistentClient
        client = PersistentClient(path=abs_vector_path)
        logger.info(f"âœ… æˆåŠŸåˆå§‹åŒ–PersistentClient")

        # åˆ é™¤æ—§é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        try:
            client.delete_collection(name="qwen_rag")
            logger.info("ğŸ”„ åˆ é™¤æ—§é›†åˆ: qwen_rag")
        except Exception as e:
            logger.info(f"â„¹ï¸ é›†åˆä¸å­˜åœ¨æˆ–æ— æ³•åˆ é™¤: {e}")

        # åˆ›å»ºç¬¦åˆChromaæ¥å£çš„åµŒå…¥å‡½æ•°é€‚é…å™¨
        class ChromaEmbeddingAdapter:
            def __init__(self, embedding_model):
                self.embedding_model = embedding_model

            def __call__(self, input):
                # ChromaæœŸæœ›inputæ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ—è¡¨
                return self.embedding_model.embed_documents(input)

        # ä½¿ç”¨é€‚é…å™¨åŒ…è£…åµŒå…¥æ¨¡å‹
        embedding_adapter = ChromaEmbeddingAdapter(embeddings)

        # åˆ›å»ºæ–°é›†åˆ
        collection = client.create_collection(
            name="qwen_rag",
            embedding_function=embedding_adapter
        )
        logger.info(f"âœ… åˆ›å»ºé›†åˆ: qwen_rag")

        # æ‰‹åŠ¨æ·»åŠ æ–‡æ¡£
        if all_chunks:
            logger.info(f"ğŸ”„ å‡†å¤‡æ·»åŠ  {len(all_chunks)} ä¸ªæ–‡æ¡£åˆ°å‘é‡åº“...")
            document_ids = [str(i) for i in range(len(all_chunks))]
            documents = [chunk.page_content for chunk in all_chunks]
            metadatas = [chunk.metadata for chunk in all_chunks]

            collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=document_ids
            )
            logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(all_chunks)} ä¸ªæ–‡æ¡£åˆ°å‘é‡åº“")
            logger.info(f"ğŸ“Š å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯: æ–‡æ¡£æ•°é‡={collection.count()}")

            # å°è¯•è·å–æ–‡æ¡£ID
            results = collection.get(limit=5)
            logger.info(f"âœ… è·å–åˆ° {len(results['ids'])} ä¸ªæ–‡æ¡£ID: {results['ids'][:5]}")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰æ–‡æ¡£å¯æ·»åŠ åˆ°å‘é‡åº“")

        # åˆ›å»ºLangChainçš„Chromaå®ä¾‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        vectorstore = Chroma(
            client=client,
            collection_name="qwen_rag",
            embedding_function=embeddings
        )
    except Exception as e:
        logger.error(f"âŒ å‘é‡åº“åˆ›å»ºå¤±è´¥: {str(e)}")
        return

    # 5. æœç´¢æµ‹è¯•
    try:
        logger.info("ğŸ” å¼€å§‹æœç´¢æµ‹è¯•")
        query = "æŠ¥å‘Šå†™äº†ä»€ä¹ˆï¼Ÿ"
        logger.info(f"ğŸ“ æœç´¢æŸ¥è¯¢: {query}")
        results = vectorstore.similarity_search(query, k=2)
        logger.info(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
        logger.info(f"\nğŸ” æœç´¢ '{query}' çš„ç»“æœï¼š")
        for i, r in enumerate(results):
            logger.info(f"{i+1}. æ¥æº: {r.metadata['source']}")
            logger.info(f"   å†…å®¹: {r.page_content[:100]}...\n")
        logger.info("âœ… æœç´¢æµ‹è¯•å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ æœç´¢æµ‹è¯•å¤±è´¥: {str(e)}", exc_info=True)

    logger.info("ğŸ ç¨‹åºæ‰§è¡Œå®Œæˆ")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
    logger.info("ğŸ ç¨‹åºæ‰§è¡Œå®Œæˆ")