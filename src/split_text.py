import json
import os
import sys
import logging
import traceback
import time
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma  # æ›´æ–°ä¸ºæ–°çš„å¯¼å…¥
from langchain_community.embeddings import DashScopeEmbeddings
from langchain.docstore.document import Document

# é…ç½®æ—¥å¿— - åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶ï¼Œä½¿ç”¨UTF-8ç¼–ç 
log_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'split_text.log')
# è®¾ç½®æ–‡ä»¶å¤„ç†å™¨ä½¿ç”¨UTF-8ç¼–ç 
file_handler = logging.FileHandler(log_file, encoding='utf-8')
# è®¾ç½®æ§åˆ¶å°å¤„ç†å™¨
stream_handler = logging.StreamHandler()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[file_handler, stream_handler]
)
logger = logging.getLogger(__name__)

# ================== é…ç½® ==================
# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„ï¼ˆæ¯”å¦‚ D:\ai-rag-app\src\split_text.pyï¼‰
current_dir = os.path.dirname(os.path.abspath(__file__))
# æ„å»ºå…¶ä»–æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
INPUT_JSON = os.path.join(current_dir, "docs.json")
# å‘é‡æ•°æ®åº“ä¿å­˜è·¯å¾„
VECTOR_DB_PATH = os.path.abspath(os.path.join(current_dir, "..", "vectorstore")) # ../vectorstore
# DashScope Embedding æ¨¡å‹
EMBEDDING_MODEL = "text-embedding-v1"

# è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡
os.environ["DASHSCOPE_API_KEY"] = "sk-7940c68d86644583bc69778b8651d7e2"

# æ£€æŸ¥ API Key æ˜¯å¦è®¾ç½®
api_key = os.getenv("DASHSCOPE_API_KEY")
if not api_key:
    logger.error("æœªæ‰¾åˆ° DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡ï¼")
    exit(1)
logger.info("API Key å·²åŠ è½½")

# ================== ä¸»ç¨‹åº ==================
def main():
    logger.info("ğŸš€ å¼€å§‹æ–‡æœ¬åˆ†å—ä¸å‘é‡åŒ–...")

    # 1. è¯»å–æ˜¨å¤©ç”Ÿæˆçš„ docs.json
    try:
        with open(INPUT_JSON, "r", encoding="utf-8") as f:
            docs = json.load(f)
        logger.info(f"âœ… åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
    except Exception as e:
        logger.error(f"è¯»å– docs.json å¤±è´¥: {e}")
        traceback.print_exc()
        exit(1)

    # 2. åˆ›å»ºæ–‡æœ¬åˆ†å—å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # æ¯å— 500 å­—ç¬¦
        chunk_overlap=50,  # å—ä¹‹é—´é‡å  50 å­—ç¬¦ï¼ˆé˜²æ­¢æ–­å¥ï¼‰
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
    )

    # 3. æå–æ–‡æœ¬å¹¶åˆ†å—
    chunked_documents = []
    total_chunks = 0

    # åœ¨åˆ†å—åï¼Œåˆ›å»º Document å¯¹è±¡
    for doc in docs:
        if doc["status"] != "success":
            logger.warning(f"è·³è¿‡çŠ¶æ€ä¸º {doc['status']} çš„æ–‡æ¡£: {doc.get('filename', 'æœªçŸ¥')}")
            continue

        content = doc["content"]
        filename = doc["filename"]

        try:
            chunks = text_splitter.split_text(content)
            logger.info(f"æ–‡æ¡£ {filename} åˆ†å—å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªå—")

            for i, chunk in enumerate(chunks):
                # ä½¿ç”¨ Document ç±»
                chunked_documents.append(
                    Document(
                        page_content=chunk,
                        metadata={
                            "source": filename,
                            "chunk_id": i,
                            "type": doc["type"]
                        }
                    )
                )
                total_chunks += 1
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£ {filename} æ—¶å‡ºé”™: {e}")
            traceback.print_exc()
            continue

    logger.info(f"âœ… æ–‡æœ¬åˆ†å—å®Œæˆï¼å…±ç”Ÿæˆ {total_chunks} ä¸ªçŸ¥è¯†å—")

    # 4. åˆ›å»ºåµŒå…¥æ¨¡å‹
    logger.info("ğŸ§  æ­£åœ¨ç”Ÿæˆå‘é‡ embeddings...ï¼ˆå¯èƒ½éœ€è¦å‡ ç§’/æ¯å— ï¼‰")
    # å°è¯•ä½¿ç”¨ONNX Runtimeæˆ–å¤‡ç”¨æ–¹æ¡ˆ
    embedding_function = None
    try:
        # å°è¯•å¯¼å…¥ONNX Runtime
        import onnxruntime
        logger.info(f"âœ… ONNX Runtimeå·²å®‰è£…ï¼Œç‰ˆæœ¬: {onnxruntime.__version__}")
        
        # ç¦ç”¨é¥æµ‹
        import os
        os.environ["CHROMA_TELEMETRY_DISABLED"] = "1"
        logger.info("âœ… å·²ç¦ç”¨é¥æµ‹")
        
        # å°è¯•è®¾ç½®ONNX Runtimeæ‰§è¡Œæä¾›ç¨‹åºä»¥æé«˜å…¼å®¹æ€§
        try:
            providers = onnxruntime.get_available_providers()
            logger.info(f"âœ… ONNX Runtimeå¯ç”¨æä¾›ç¨‹åº: {providers}")
            # ä½¿ç”¨CPUæä¾›ç¨‹åºä½œä¸ºå¤‡é€‰
            if "CPUExecutionProvider" in providers:
                os.environ["ONNX_RUNTIME_EXECUTION_PROVIDER"] = "CPUExecutionProvider"
                logger.info("âœ… å·²è®¾ç½®ONNX Runtimeæ‰§è¡Œæä¾›ç¨‹åºä¸ºCPUExecutionProvider")
        except Exception as ep_e:
            logger.warning(f"è®¾ç½®ONNX Runtimeæ‰§è¡Œæä¾›ç¨‹åºæ—¶å‡ºé”™: {ep_e}")
        
        # ä½¿ç”¨ONNX RuntimeåµŒå…¥å‡½æ•°
        try:
            from chromadb.utils import embedding_functions
            # å°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹æˆ–ç¦ç”¨è¿œç¨‹åŠ è½½
            import os
            os.environ["TRANSFORMERS_OFFLINE"] = "1"
            logger.info("âœ… å·²å¯ç”¨ç¦»çº¿æ¨¡å¼")
            embedding_function = embedding_functions.OnnxEmbeddingFunction()
            logger.info("âœ… ä½¿ç”¨ONNX RuntimeåµŒå…¥å‡½æ•°")
        except Exception as onnx_e:
            logger.error(f"âŒ åˆå§‹åŒ–ONNX RuntimeåµŒå…¥å‡½æ•°å¤±è´¥: {str(onnx_e)}")
            logger.info("ğŸ”„ å°è¯•ä½¿ç”¨ç®€å•çš„åµŒå…¥å‡½æ•°æ›¿ä»£æ–¹æ¡ˆ")
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„åµŒå…¥å‡½æ•°åŒ…è£…ç±»ï¼Œæä¾›name()æ–¹æ³•
            class SimpleEmbeddingFunctionWrapper:
                def __init__(self, embedding_function):
                    self.embedding_function = embedding_function
                
                def __call__(self, input):
                    return self.embedding_function(input)
                
                def name(self):
                    return "simple_embedding_function"
            
            # ç®€å•çš„åŸºäºå­—ç¬¦é•¿åº¦çš„åµŒå…¥ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
            def simple_embedding_function(texts):
                return [[len(text)] for text in texts]
            
            # ä½¿ç”¨åŒ…è£…ç±»åŒ…è£…åµŒå…¥å‡½æ•°
            embedding_function = SimpleEmbeddingFunctionWrapper(simple_embedding_function)
            logger.info("âœ… ä½¿ç”¨ç®€å•çš„åµŒå…¥å‡½æ•°æ›¿ä»£æ–¹æ¡ˆï¼ˆå¸¦name()æ–¹æ³•çš„åŒ…è£…ç±»ï¼‰")
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥ONNX Runtimeå¤±è´¥: {str(e)}")
        logger.error("å°è¯•å®‰è£…ä¾èµ–: pip install onnxruntime")
        raise
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–ONNX Runtimeæ—¶å‘ç”Ÿå…¶ä»–é”™è¯¯: {str(e)}")
        logger.error("å°è¯•ä½¿ç”¨sentence-transformersä½œä¸ºå¤‡é€‰åµŒå…¥æ¨¡å‹")
        
        # å°è¯•ä½¿ç”¨sentence-transformersä½œä¸ºå¤‡é€‰
        try:
            from chromadb.utils import embedding_functions
            embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name="all-MiniLM-L6-v2"
            )
            logger.info("âœ… å·²åˆ‡æ¢åˆ°sentence-transformersåµŒå…¥æ¨¡å‹")
        except Exception as st_e:
            logger.error(f"âŒ åˆå§‹åŒ–sentence-transformersåµŒå…¥æ¨¡å‹å¤±è´¥: {str(st_e)}")
            logger.error("å°è¯•å®‰è£…ä¾èµ–: pip install sentence-transformers")
            raise

    # 5. åˆ†æ­¥ç”Ÿæˆå‘é‡å¹¶å­˜å…¥ Chroma
    logger.info("ğŸ§  æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")

    # ä¿®æ”¹chromadbåˆå§‹åŒ–éƒ¨åˆ†çš„ä»£ç å¦‚ä¸‹

    # åˆå§‹åŒ–chromadbå®¢æˆ·ç«¯
    try:
        import chromadb
        logger.info("æ­£åœ¨åˆå§‹åŒ–chromadbå®¢æˆ·ç«¯...")
        client = chromadb.EphemeralClient()  # ä½¿ç”¨å†…å­˜æ¨¡å¼ï¼Œé¿å…é…ç½®é—®é¢˜
        logger.info("âœ… chromadbå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆå†…å­˜æ¨¡å¼ï¼‰")

        # åˆ›å»ºæˆ–è·å–é›†åˆ
        collection_name = "document_collection"
        logger.info(f"æ­£åœ¨åˆ›å»ºæˆ–è·å–é›†åˆ: {collection_name}")
        collection = client.get_or_create_collection(
            name=collection_name,
            embedding_function=embedding_function  # âœ… å…³é”®ï¼šæŠŠä½ çš„ embedding_function ä¼ è¿›å»
        )
        logger.info(f"âœ… æˆåŠŸåˆ›å»º/è·å–é›†åˆ: {collection_name}")

    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–chromadbå¤±è´¥: {type(e).__name__}: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        exit(1)

    # å°è¯•æ·»åŠ æ–‡æ¡£ï¼ˆç®€åŒ–ç‰ˆï¼‰
    try:
        logger.info("å‡†å¤‡æ·»åŠ æ–‡æ¡£åˆ°é›†åˆ...")
        # åªå–ç¬¬ä¸€ä¸ªæ–‡æ¡£è¿›è¡Œæµ‹è¯•
        if chunked_documents:
            doc = chunked_documents[0]
            logger.info(f"æµ‹è¯•æ·»åŠ ç¬¬ä¸€ä¸ªçŸ¥è¯†å—: '{doc.page_content[:50]}...'")

            # å‡†å¤‡æ•°æ®
            doc_id = "test_doc_1"
            documents = [doc.page_content]
            metadatas = [doc.metadata]
            ids = [doc_id]

            logger.info(f"æ–‡æ¡£æ•°æ®å‡†å¤‡å®Œæ¯•ï¼ŒID: {doc_id}")
            logger.info(f"æ–‡æ¡£å†…å®¹å‰50å­—ç¬¦: {doc.page_content[:50]}...")
            logger.info(f"å…ƒæ•°æ®: {metadatas[0]}")

            # å°è¯•æ·»åŠ æ–‡æ¡£
            logger.info(f"å¼€å§‹æ‰§è¡Œcollection.add()ï¼ŒID: {doc_id}")
            start_add = time.time()
            try:
                collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                end_add = time.time()
                logger.info(f"âœ… collection.add()æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {end_add - start_add:.2f}ç§’")

                # éªŒè¯æ·»åŠ ç»“æœ
                logger.info("å¼€å§‹éªŒè¯æ·»åŠ ç»“æœ...")
                try:
                    count = collection.count()
                    logger.info(f"âœ… é›†åˆä¸­çš„å‘é‡æ•°é‡: {count}")

                    # å°è¯•æ£€ç´¢
                    retrieved = collection.get(ids=[doc_id])
                    if len(retrieved['ids']) > 0:
                        logger.info(f"âœ… æˆåŠŸæ£€ç´¢åˆ°æ·»åŠ çš„æ–‡æ¡£ï¼ŒID: {retrieved['ids'][0]}")
                        logger.info(f"æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹: {retrieved['documents'][0][:50]}...")
                        logger.info(f"æ£€ç´¢åˆ°çš„å…ƒæ•°æ®: {retrieved['metadatas'][0]}")
                    else:
                        logger.warning(f"âš ï¸ æœªèƒ½æ£€ç´¢åˆ°æ·»åŠ çš„æ–‡æ¡£ï¼ŒID: {doc_id}")
                except Exception as verify_e:
                    logger.error(f"âŒ éªŒè¯æ·»åŠ ç»“æœæ—¶å‡ºé”™: {type(verify_e).__name__}: {str(verify_e)}")
                    logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            except Exception as add_e:
                logger.error(f"âŒ æ‰§è¡Œcollection.add()æ—¶å‡ºé”™: {type(add_e).__name__}: {str(add_e)}")
                logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                # å°è¯•æ‰“å°åµŒå…¥å‡½æ•°ä¿¡æ¯ï¼Œæ’æŸ¥é—®é¢˜
                logger.info(f"åµŒå…¥å‡½æ•°ç±»å‹: {type(embedding_function)}")
                logger.info(f"åµŒå…¥å‡½æ•°: {embedding_function}")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯æ·»åŠ çš„æ–‡æ¡£")
    except Exception as e:
        logger.error(f"âŒ æ·»åŠ æ–‡æ¡£åˆ°é›†åˆæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {type(e).__name__}: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

    # æ–‡æ¡£æ·»åŠ å®Œæˆ
    logger.info("âœ… æ–‡æ¡£æ·»åŠ æµç¨‹å·²å®Œæˆ")

    logger.info("âœ… å‘é‡æ•°æ®åº“æ“ä½œå®Œæˆ (ç®€åŒ–æµ‹è¯•æ¨¡å¼)")

    # 6. æµ‹è¯•æœç´¢ (å¦‚æœæ·»åŠ æˆåŠŸ)
    if 'collection' in locals():
        logger.info("\n==============================")
        logger.info("ğŸ” æµ‹è¯•è¯­ä¹‰æœç´¢...")

        query = "æŠ¥å‘Šå†™äº†ä»€ä¹ˆï¼Ÿ"
        try:
            # ä½¿ç”¨collectionè¿›è¡Œæœç´¢
            results = collection.query(
                query_texts=[query],
                n_results=2
            )
            if results and 'documents' in results and len(results['documents']) > 0:
                for i, doc in enumerate(results['documents'][0]):
                    logger.info(f"\nğŸ¯ åŒ¹é… {i + 1}:")
                    logger.info(f"ğŸ‘‰ {doc[:200]}...")
            else:
                logger.info("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…ç»“æœ")
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥ï¼é”™è¯¯: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    else:
        logger.info("âš ï¸ æœªåˆ›å»ºé›†åˆï¼Œè·³è¿‡æœç´¢æµ‹è¯•")


if __name__ == "__main__":
    main()
